{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Voice Symptom Intake & Documentation Assistant - Google Colab Deployment\n",
                "\n",
                "This notebook deploys the Voice Symptom Intake & Documentation Assistant on Google Colab with GPU support.\n",
                "\n",
                "## ‚úÖ Advantages of Colab Deployment:\n",
                "- **Free GPU Access** (Tesla T4 with 16GB VRAM)\n",
                "- **No Local Setup Issues** (no Windows file locking, FFmpeg, etc.)\n",
                "- **Faster Inference** (MedASR & MedGemma both run on GPU)\n",
                "- **Public URL Access** via ngrok\n",
                "\n",
                "## ‚ö†Ô∏è Important Notes:\n",
                "- Sessions last up to 12 hours (free tier)\n",
                "- You'll need a **Hugging Face token** with \"Read\" access\n",
                "- Accept model terms for `google/medasr` and `google/medgemma-1.5-4b-it`\n",
                "\n",
                "## üÜï Latest Updates:\n",
                "- Enhanced JSON parsing for reliable documentation generation\n",
                "- Robust error recovery with intelligent fallback strategies\n",
                "- Optimized generation parameters for medical documentation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Check GPU Availability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Install Dependencies\n",
                "\n",
                "Install all required packages (this takes ~3-5 minutes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "# Install transformers from specific commit for MedASR support\n",
                "!pip install git+https://github.com/huggingface/transformers.git@65dc261512cbdb1ee72b88ae5b222f2605aad8e5\n",
                "\n",
                "# Install other dependencies\n",
                "!pip install fastapi uvicorn[standard] python-multipart\n",
                "!pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "!pip install accelerate librosa soundfile noisereduce audioread\n",
                "!pip install pydantic pydantic-settings python-dotenv\n",
                "!pip install pyngrok nest-asyncio\n",
                "\n",
                "print(\"‚úÖ All dependencies installed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Upload Your Code\n",
                "\n",
                "**Option A:** Upload project folder from your computer\n",
                "- Click the folder icon on the left sidebar\n",
                "- Upload the entire `voice-symptom-triage-assistant` folder\n",
                "\n",
                "**Option B:** Clone from GitHub (if you've pushed your code)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option B: Clone from GitHub (uncomment and modify if using)\n",
                "# !git clone https://github.com/YOUR_USERNAME/voice-symptom-triage-assistant.git\n",
                "# %cd voice-symptom-triage-assistant\n",
                "\n",
                "# Option A: If uploaded manually, navigate to the folder\n",
                "%cd /content/voice-symptom-triage-assistant"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Configure Hugging Face Token & Environment\n",
                "\n",
                "**Get your token from:** https://huggingface.co/settings/tokens\n",
                "\n",
                "**Make sure you've accepted terms for:**\n",
                "- https://huggingface.co/google/medasr\n",
                "- https://huggingface.co/google/medgemma-1.5-4b-it"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# REPLACE WITH YOUR HUGGING FACE TOKEN\n",
                "HF_TOKEN = \"hf_YOUR_TOKEN_HERE\"\n",
                "\n",
                "# Create .env file with enhanced MedGemma parameters\n",
                "with open('.env', 'w') as f:\n",
                "    f.write(f\"HF_TOKEN={HF_TOKEN}\\n\")\n",
                "    f.write(\"MEDASR_MODEL=google/medasr\\n\")\n",
                "    f.write(\"MEDGEMMA_MODEL=google/medgemma-1.5-4b-it\\n\")\n",
                "    f.write(\"DEVICE=cuda\\n\")\n",
                "    f.write(\"ENABLE_GPU=true\\n\")\n",
                "    \n",
                "    # MedGemma Generation Parameters (optimized for JSON output)\n",
                "    f.write(\"MEDGEMMA_TEMPERATURE=0.1\\n\")\n",
                "    f.write(\"MEDGEMMA_MAX_TOKENS=1024\\n\")\n",
                "    f.write(\"MEDGEMMA_REPETITION_PENALTY=1.1\\n\")\n",
                "    \n",
                "    # Audio settings\n",
                "    f.write(\"AUDIO_SAMPLE_RATE=16000\\n\")\n",
                "    f.write(\"MAX_AUDIO_DURATION_SECONDS=300\\n\")\n",
                "    \n",
                "    # Model cache directory\n",
                "    f.write(\"MODEL_CACHE_DIR=./models\\n\")\n",
                "    f.write(\"LOG_LEVEL=INFO\\n\")\n",
                "\n",
                "print(\"‚úÖ Environment configured with enhanced MedGemma parameters!\")\n",
                "print(\"   - Temperature: 0.1 (deterministic JSON output)\")\n",
                "print(\"   - Max Tokens: 1024 (complete documentation)\")\n",
                "print(\"   - Repetition Penalty: 1.1 (prevent loops)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Enable GPU for MedGemma (Colab T4 has 16GB VRAM!)\n",
                "\n",
                "Update MedGemma service to use GPU acceleration on Colab.\n",
                "\n",
                "**Note:** Your local code is optimized for CPU due to limited VRAM, but Colab can handle GPU inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Update medgemma_service.py to enable GPU on Colab\n",
                "import re\n",
                "\n",
                "# Read the current service file\n",
                "with open('app/models/medgemma_service.py', 'r') as f:\n",
                "    content = f.read()\n",
                "\n",
                "# Replace the CPU-forced loading with GPU-capable loading\n",
                "gpu_loading_code = '''        try:\n",
                "            logger.info(f\"Loading MedGemma model on device: {self.device}\")\n",
                "            \n",
                "            # Colab T4 GPU has 16GB VRAM - can fit MedGemma with float16!\n",
                "            dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
                "            \n",
                "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
                "                settings.medgemma_model,\n",
                "                token=settings.hf_token if settings.hf_token else None\n",
                "            )\n",
                "            \n",
                "            self.model = AutoModelForCausalLM.from_pretrained(\n",
                "                settings.medgemma_model,\n",
                "                torch_dtype=dtype,\n",
                "                device_map=\"auto\" if settings.enable_gpu else None,\n",
                "                token=settings.hf_token if settings.hf_token else None,\n",
                "                low_cpu_mem_usage=True\n",
                "            )\n",
                "            \n",
                "            if not settings.enable_gpu:\n",
                "                self.model = self.model.to(\"cpu\")\n",
                "                self.device = \"cpu\"\n",
                "            \n",
                "            self.model.eval()\n",
                "            \n",
                "            logger.info(f\"MedGemma model loaded successfully on {self.device}\")\n",
                "'''\n",
                "\n",
                "# Find and replace the _load_model method's model loading section\n",
                "# This replaces from \"try:\" up to the model.eval() call\n",
                "pattern = r'(\\s+def _load_model\\(self\\):[^\\n]+\\n[^\\n]+\\n\\s+try:).*?(\\s+logger\\.info\\(\"MedGemma model loaded successfully\"\\))'\n",
                "replacement = r'\\1' + gpu_loading_code + r'\\2'\n",
                "\n",
                "content_updated = re.sub(pattern, replacement, content, flags=re.DOTALL)\n",
                "\n",
                "# Write back\n",
                "with open('app/models/medgemma_service.py', 'w') as f:\n",
                "    f.write(content_updated)\n",
                "\n",
                "print(\"‚úÖ MedGemma service updated for GPU acceleration!\")\n",
                "print(\"   Model will use float16 precision on T4 GPU\")\n",
                "print(\"   Expected speedup: 5-10x faster than CPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Set Up ngrok Tunnel\n",
                "\n",
                "**Get your ngrok authtoken from:** https://dashboard.ngrok.com/get-started/your-authtoken"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyngrok import ngrok\n",
                "import nest_asyncio\n",
                "\n",
                "# REPLACE WITH YOUR NGROK AUTHTOKEN\n",
                "NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"\n",
                "\n",
                "# Set up ngrok\n",
                "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
                "nest_asyncio.apply()\n",
                "\n",
                "print(\"‚úÖ ngrok configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Start the Server\n",
                "\n",
                "This will:\n",
                "1. Load MedASR model on GPU (~30 seconds)\n",
                "2. Load MedGemma model on GPU (~2-3 minutes first time)\n",
                "3. Start the FastAPI server\n",
                "4. Create a public ngrok URL\n",
                "\n",
                "**Note:** With the enhanced JSON parsing, documentation generation is now much more reliable!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import threading\n",
                "import time\n",
                "\n",
                "# Start ngrok tunnel\n",
                "public_url = ngrok.connect(8000)\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üåê PUBLIC URL: {public_url}\")\n",
                "print(f\"{'='*60}\\n\")\n",
                "print(\"Open this URL in your browser to access the application!\\n\")\n",
                "print(\"üÜï Features in this deployment:\")\n",
                "print(\"   ‚úì Enhanced JSON parsing for reliable documentation\")\n",
                "print(\"   ‚úì Intelligent fallback if JSON parsing fails\")\n",
                "print(\"   ‚úì Optimized generation parameters\")\n",
                "print(\"   ‚úì GPU acceleration for faster results\\n\")\n",
                "\n",
                "# Start uvicorn server\n",
                "!python -m uvicorn app.main:app --host 0.0.0.0 --port 8000"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Access Your Application\n",
                "\n",
                "1. **Copy the public ngrok URL** from the output above\n",
                "2. **Open it in your browser**\n",
                "3. **Start recording** or **upload audio files**\n",
                "4. **View transcription and documentation results**\n",
                "\n",
                "## ‚úÖ Features:\n",
                "- Audio recording directly in browser\n",
                "- File upload (WAV, MP3, M4A, FLAC, OGG)\n",
                "- Real-time transcription with MedASR\n",
                "- Structured documentation with MedGemma\n",
                "- **Enhanced JSON parsing** - no more \"N/A\" fields!\n",
                "- **Robust error recovery** - always generates documentation\n",
                "- Export results as JSON\n",
                "- Copy to clipboard\n",
                "\n",
                "## üõë To Stop:\n",
                "- Click the **Stop** button in Colab\n",
                "- Or press **Ctrl+C** in the cell output\n",
                "\n",
                "## üìù Notes:\n",
                "- Free Colab sessions last **up to 12 hours**\n",
                "- The ngrok URL **changes each time** you restart\n",
                "- Models are **cached** after first download (faster restarts)\n",
                "- GPU inference is **10x faster** than CPU\n",
                "- Documentation fields should now populate correctly (check logs for parsing_method)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Troubleshooting\n",
                "\n",
                "### If models fail to load:\n",
                "1. Check your HF_TOKEN is valid\n",
                "2. Verify you accepted model terms on Hugging Face\n",
                "3. Try restarting the runtime: Runtime ‚Üí Restart runtime\n",
                "\n",
                "### If ngrok fails:\n",
                "1. Verify your ngrok authtoken\n",
                "2. Free ngrok accounts have limits (1 tunnel at a time)\n",
                "3. Try getting a new authtoken from ngrok dashboard\n",
                "\n",
                "### If audio fails:\n",
                "1. Check browser microphone permissions\n",
                "2. Hard refresh browser (Ctrl+Shift+R)\n",
                "3. Ensure audio files are under 5 minutes (default limit)\n",
                "\n",
                "### If documentation shows \"N/A\" fields:\n",
                "1. Check the server logs for \"parsing_method\" messages\n",
                "2. Look for \"json_successful\" or \"text_extraction_fallback\"\n",
                "3. If seeing frequent fallback, the model may need more warmup time\n",
                "4. Check logs for any JSON extraction warnings\n",
                "\n",
                "### Debug Mode:\n",
                "If you want to see detailed MedGemma output for debugging:\n",
                "```python\n",
                "# In a new cell\n",
                "import logging\n",
                "logging.getLogger('app.models.medgemma_service').setLevel(logging.DEBUG)\n",
                "```\n",
                "Then restart the server to see detailed parsing logs."
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}