{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Voice Symptom Intake & Documentation Assistant - Google Colab Deployment\n",
                "\n",
                "This notebook deploys the Voice Symptom Intake & Documentation Assistant on Google Colab with GPU support.\n",
                "\n",
                "## ‚úÖ Advantages of Colab Deployment:\n",
                "- **Free GPU Access** (Tesla T4 with 16GB VRAM)\n",
                "- **No Local Setup Issues** (no Windows file locking, FFmpeg, etc.)\n",
                "- **Faster Inference** (MedASR & MedGemma both run on GPU)\n",
                "- **Public URL Access** via ngrok\n",
                "\n",
                "## ‚ö†Ô∏è Important Notes:\n",
                "- Sessions last up to 12 hours (free tier)\n",
                "- You'll need a **Hugging Face token** with \"Read\" access\n",
                "- Accept model terms for `google/medasr` and `google/medgemma-1.5-4b-it`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Check GPU Availability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Install Dependencies\n",
                "\n",
                "Install all required packages (this takes ~3-5 minutes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "# Install transformers from specific commit for MedASR support\n",
                "!pip install git+https://github.com/huggingface/transformers.git@65dc261512cbdb1ee72b88ae5b222f2605aad8e5\n",
                "\n",
                "# Install other dependencies\n",
                "!pip install fastapi uvicorn[standard] python-multipart\n",
                "!pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "!pip install accelerate librosa soundfile noisereduce audioread\n",
                "!pip install pydantic pydantic-settings python-dotenv\n",
                "!pip install pyngrok nest-asyncio\n",
                "\n",
                "print(\"‚úÖ All dependencies installed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Upload Your Code\n",
                "\n",
                "**Option A:** Upload project folder from your computer\n",
                "- Click the folder icon on the left sidebar\n",
                "- Upload the entire `voice-symptom-triage-assistant` folder\n",
                "\n",
                "**Option B:** Clone from GitHub (if you've pushed your code)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option B: Clone from GitHub (uncomment and modify if using)\n",
                "# !git clone https://github.com/YOUR_USERNAME/voice-symptom-triage-assistant.git\n",
                "# %cd voice-symptom-triage-assistant\n",
                "\n",
                "# Option A: If uploaded manually, navigate to the folder\n",
                "%cd /content/voice-symptom-triage-assistant"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Configure Hugging Face Token\n",
                "\n",
                "**Get your token from:** https://huggingface.co/settings/tokens\n",
                "\n",
                "**Make sure you've accepted terms for:**\n",
                "- https://huggingface.co/google/medasr\n",
                "- https://huggingface.co/google/medgemma-1.5-4b-it"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# REPLACE WITH YOUR HUGGING FACE TOKEN\n",
                "HF_TOKEN = \"hf_YOUR_TOKEN_HERE\"\n",
                "\n",
                "# Create .env file\n",
                "with open('.env', 'w') as f:\n",
                "    f.write(f\"HF_TOKEN={HF_TOKEN}\\n\")\n",
                "    f.write(\"MEDASR_MODEL=google/medasr\\n\")\n",
                "    f.write(\"MEDGEMMA_MODEL=google/medgemma-1.5-4b-it\\n\")\n",
                "    f.write(\"DEVICE=cuda\\n\")\n",
                "    f.write(\"ENABLE_GPU=true\\n\")\n",
                "    f.write(\"AUDIO_SAMPLE_RATE=16000\\n\")\n",
                "    f.write(\"MAX_AUDIO_DURATION_SECONDS=300\\n\")\n",
                "    f.write(\"MODEL_CACHE_DIR=./models\\n\")\n",
                "    f.write(\"LOG_LEVEL=INFO\\n\")\n",
                "\n",
                "print(\"‚úÖ Environment configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Update MedGemma for GPU (Colab has 16GB VRAM!)\n",
                "\n",
                "Since Colab has more VRAM than GTX 1650, we can run MedGemma on GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Update medgemma_service.py to use GPU\n",
                "medgemma_fix = '''\n",
                "            # Colab GPU has 16GB VRAM - can fit MedGemma!\n",
                "            dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
                "            \n",
                "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
                "                settings.medgemma_model,\n",
                "                token=settings.hf_token if settings.hf_token else None\n",
                "            )\n",
                "            \n",
                "            self.model = AutoModelForCausalLM.from_pretrained(\n",
                "                settings.medgemma_model,\n",
                "                torch_dtype=dtype,\n",
                "                device_map=\"auto\" if settings.enable_gpu else None,\n",
                "                token=settings.hf_token if settings.hf_token else None\n",
                "            )\n",
                "            \n",
                "            self.model.eval()\n",
                "'''\n",
                "\n",
                "print(\"Note: MedGemma will use GPU on Colab (16GB VRAM available)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Set Up ngrok Tunnel\n",
                "\n",
                "**Get your ngrok authtoken from:** https://dashboard.ngrok.com/get-started/your-authtoken"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyngrok import ngrok\n",
                "import nest_asyncio\n",
                "\n",
                "# REPLACE WITH YOUR NGROK AUTHTOKEN\n",
                "NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"\n",
                "\n",
                "# Set up ngrok\n",
                "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
                "nest_asyncio.apply()\n",
                "\n",
                "print(\"‚úÖ ngrok configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Start the Server\n",
                "\n",
                "This will:\n",
                "1. Load MedASR model (~30 seconds)\n",
                "2. Load MedGemma model (~2-3 minutes first time)\n",
                "3. Start the FastAPI server\n",
                "4. Create a public ngrok URL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import threading\n",
                "import time\n",
                "\n",
                "# Start ngrok tunnel\n",
                "public_url = ngrok.connect(8000)\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üåê PUBLIC URL: {public_url}\")\n",
                "print(f\"{'='*60}\\n\")\n",
                "print(\"Open this URL in your browser to access the application!\\n\")\n",
                "\n",
                "# Start uvicorn server\n",
                "!python -m uvicorn app.main:app --host 0.0.0.0 --port 8000"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Access Your Application\n",
                "\n",
                "1. **Copy the public ngrok URL** from the output above\n",
                "2. **Open it in your browser**\n",
                "3. **Start recording** or **upload audio files**\n",
                "4. **View transcription and documentation results**\n",
                "\n",
                "## ‚úÖ Features:\n",
                "- Audio recording directly in browser\n",
                "- File upload (WAV, MP3, M4A, FLAC, OGG)\n",
                "- Real-time transcription with MedASR\n",
                "- Structured documentation with MedGemma\n",
                "- Export results as JSON\n",
                "- Copy to clipboard\n",
                "\n",
                "## üõë To Stop:\n",
                "- Click the **Stop** button in Colab\n",
                "- Or press **Ctrl+C** in the cell output\n",
                "\n",
                "## üìù Notes:\n",
                "- Free Colab sessions last **up to 12 hours**\n",
                "- The ngrok URL **changes each time** you restart\n",
                "- Models are **cached** after first download (faster restarts)\n",
                "- GPU inference is **10x faster** than CPU"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Troubleshooting\n",
                "\n",
                "### If models fail to load:\n",
                "1. Check your HF_TOKEN is valid\n",
                "2. Verify you accepted model terms on Hugging Face\n",
                "3. Try restarting the runtime: Runtime ‚Üí Restart runtime\n",
                "\n",
                "### If ngrok fails:\n",
                "1. Verify your ngrok authtoken\n",
                "2. Free ngrok accounts have limits (1 tunnel at a time)\n",
                "3. Try getting a new authtoken from ngrok dashboard\n",
                "\n",
                "### If audio fails:\n",
                "1. Check browser microphone permissions\n",
                "2. Hard refresh browser (Ctrl+Shift+R)\n",
                "3. Ensure audio files are under 5 minutes (default limit)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}